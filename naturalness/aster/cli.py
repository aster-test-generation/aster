
from enum import Enum
from pathlib import Path

import typer
from typing_extensions import Annotated
from aster.evaluate import EvaluateTest
from aster.utils import constants
from aster.utils.richlog import RichLog


app = typer.Typer(
    help="ASTER Naturalness Evaluator",
    pretty_exceptions_enable=False,
    pretty_exceptions_show_locals=False,
    add_completion=False,
)



class EvaluationType(str, Enum):
    """Defines the types of evaluations that can be done"""

    naturalness = "naturalness"


class GranularityType(str, Enum):
    """Defines the granularity at which tests are generated"""

    clazz = "class"
    method = "method"


class OutputFormatType(str, Enum):
    """Defines the allowable output formats"""

    json = "json"
    jpg = "jpg"
    pdf = "pdf"
    png = "png"
    list = "list"
    mean_median = "mean_median"




@app.command()
def evaluate(
    project_root: Annotated[
        str,
        typer.Option(
            help="Path to the root directory of the application under test.",
            show_default=False,
        ),
    ],
    type: Annotated[
        EvaluationType,
        typer.Option(help="Evaluation type.", show_choices=True),
    ] = EvaluationType.naturalness,
    generated_by: Annotated[
        constants.GeneratedType,
        typer.Option(
            help="Evaluate individually or compare tests generated by ASTER, developers, and EvoSuite.",
            show_choices=True,
        ),
    ] = constants.GeneratedType.aster,
    target_class: Annotated[
        str,
        typer.Option(help="Focal class (fully qualified class name)."),
    ] = "all",
    granularity: Annotated[
        GranularityType,
        typer.Option(
            help="Naturalness score generated based on the selected granularity level (class, method).",
            show_choices=True,
        ),
    ] = GranularityType.method,
    output_path: Annotated[
        str,
        typer.Option(help="Output directory where the evaluation plots are created."),
    ] = str(Path(constants.EVALUATE_OUTPUT_DIR)),
    output_format: Annotated[
        OutputFormatType,
        typer.Option(
            help="Output format (JSON, JPEG, PNG, PDF. For any option other than JSON, an interactive"
            "graph will be created and opened in your default browser).",
            show_choices=True,
        ),
    ] = OutputFormatType.pdf,
    eager_analysis: Annotated[
        bool,
        typer.Option(help="Eagerly analyze the project"),
    ] = False,
) -> None:
    """Evaluate naturalness of the generated tests by ASTER and compare them with tests written by
    developers or generated by other tools."""
    try:
        if not project_root:
            RichLog.error(f"Please provide project root option")
            return
        match type:
            case EvaluationType.naturalness:
                EvaluateTest(
                    project_root=project_root, is_analysis_need=eager_analysis
                ).evaluate_naturalness(
                    generated_by=generated_by.name,
                    target_class=target_class,
                    granularity=granularity.name,
                    output_path=Path(output_path),
                    output_format=output_format.name,
                )
            case _:
                RichLog.error(f"Invalid evaluation type {type} selected")
                return
    finally:
        pass


if __name__ == "__main__":
    import multiprocessing

    multiprocessing.freeze_support()

    app()
